{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import cv2\n",
    "import copy \n",
    "sys.path.insert(1, '../../src/')\n",
    "import matplotlib.pyplot as plt\n",
    "from wholebodypose.models.rtmpose.model import RTMPoseModel\n",
    "from wholebodypose.utils.vision import DrawerPose\n",
    "draw_skeleton = DrawerPose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0;93m2025-07-17 23:22:37.603494965 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\u001b[m\n",
      "\u001b[0;93m2025-07-17 23:22:37.603557753 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\u001b[m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load /home/va0831/.cache/rtmlib/hub/checkpoints/yolox_m_8xb8-300e_humanart-c2c7a14a.onnx with onnxruntime backend\n",
      "load /home/va0831/.cache/rtmlib/hub/checkpoints/rtmw-dw-x-l_simcc-cocktail14_270e-384x288_20231122.onnx with onnxruntime backend\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0;93m2025-07-17 23:22:38.599508535 [W:onnxruntime:, graph.cc:3593 CleanUnusedInitializersAndNodeArgs] Removing initializer '1701'. It is not used by any node and should be removed from the model.\u001b[m\n",
      "\u001b[0;93m2025-07-17 23:22:38.599574689 [W:onnxruntime:, graph.cc:3593 CleanUnusedInitializersAndNodeArgs] Removing initializer '1706'. It is not used by any node and should be removed from the model.\u001b[m\n",
      "\u001b[0;93m2025-07-17 23:22:38.599600688 [W:onnxruntime:, graph.cc:3593 CleanUnusedInitializersAndNodeArgs] Removing initializer '1709'. It is not used by any node and should be removed from the model.\u001b[m\n",
      "\u001b[0;93m2025-07-17 23:22:38.699982036 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\u001b[m\n",
      "\u001b[0;93m2025-07-17 23:22:38.700040255 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\u001b[m\n"
     ]
    }
   ],
   "source": [
    "kpt_thr = 2.5\n",
    "model = RTMPoseModel(mode='performance',backend='onnxruntime',\n",
    "                     use_thresholding=False,filter_noise=True,kpt_thr=kpt_thr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "import numpy as np\n",
    "\n",
    "# define a video capture object \n",
    "#vid = cv2.VideoCapture(\"/media/cristian/12FF1F6D0CD48422/Research/Gloss/Gloss/Datasets/wlasl-complete/videos/17165.mp4\")#68508.mp4\")#54563.mp4\")#68914.mp4\") \n",
    "\n",
    "#vid = cv2.VideoCapture(\"/media/cristian/12FF1F6D0CD48422/Research/Gloss/Gloss/Datasets/WLASL/wlasl-complete-21k/videos/17165.mp4\")\n",
    "vid = cv2.VideoCapture(\"/data/cristian/paper_2025/WLASL_videos/WLASL/videos/17165.mp4\")\n",
    "\n",
    "                       \n",
    "#vid = cv2.VideoCapture(0)\n",
    "cv2.namedWindow(\"frame\", cv2.WINDOW_NORMAL) \n",
    "\n",
    "list_keypoints = []\n",
    "list_scores = []\n",
    "cnt = 0\n",
    "\n",
    "while(True): \n",
    "    \n",
    "    ret, frame = vid.read() \n",
    "    if ret is None or frame is None:\n",
    "        break\n",
    "    \n",
    "    frame = cv2.resize(frame,(640,480))\n",
    "    \n",
    "    try:\n",
    "        frame_rgb = cv2.cvtColor(frame,cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        keypoints, scores  = model.predict(frame_rgb)\n",
    "        list_keypoints.append(keypoints[0,:,:])\n",
    "        list_scores.append(scores[0,:])\n",
    "\n",
    "\n",
    "        frame_original = draw_skeleton(copy.deepcopy(frame), keypoints, scores, kpt_thr=kpt_thr,\n",
    "                                    line_width=1,radius=1)\n",
    "        \n",
    "       \n",
    "        score_value = np.round(scores[0,99],2)\n",
    "        key_p1 = np.round(keypoints[0,99,0],2)\n",
    "        key_p2 = np.round(keypoints[0,99,1],2)\n",
    "        cnt+=1        \n",
    "\n",
    "        frame_original = cv2.putText(frame_original, \n",
    "                    f'point 99:i={cnt},s={str(score_value)[:4]},k={str(key_p1)[:6]},{str(key_p2)[:6]}', \n",
    "                    (10, 30) ,cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 0) , 2, cv2.LINE_AA) \n",
    "        \n",
    "        #cv2.imshow('frame', frame_original) \n",
    "        #cv2.waitKey()\n",
    "        \n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'): \n",
    "            break\n",
    "    except:\n",
    "        print(traceback.format_exc())\n",
    "        break\n",
    "    \n",
    "# After the loop release the cap object \n",
    "vid.release() \n",
    "# Destroy all the windows \n",
    "cv2.destroyAllWindows() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_keypoints = np.array(list_keypoints)\n",
    "list_scores = np.array(list_scores)\n",
    "list_keypoints.shape,list_scores.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from padasip.filters import AdaptiveFilter,FilterNLMS\n",
    "\n",
    "def initialize_adaptive_filter(num_keypoints,mu=0.9):\n",
    "    filters = []\n",
    "    for _ in range(num_keypoints):\n",
    "        #filter_x = AdaptiveFilter(n=1, mu=0.00005, w=\"zeros\")\n",
    "        #filter_y = AdaptiveFilter(n=1, mu=0.00005, w=\"zeros\")\n",
    "        \n",
    "        filter_x = FilterNLMS(2, mu=mu,w=\"random\")\n",
    "        filter_y = FilterNLMS(2, mu=mu,w=\"random\")\n",
    "        filter_z = FilterNLMS(2, mu=mu,w=\"random\")\n",
    "\n",
    "        filters.append((filter_x, filter_y,filter_z))\n",
    "    return filters\n",
    "\n",
    "def track_keypoints(t,keypoints,scores, adaptive_filters):\n",
    "    keypoints_pred = np.zeros_like(keypoints)\n",
    "    scores_pred    = np.zeros_like(scores)\n",
    "\n",
    "    for i, keypoint in enumerate(keypoints):\n",
    "        filter_x, filter_y, filter_z = adaptive_filters[i]\n",
    "        #if keypoint[0] == 0 and keypoint[1] == 0:\n",
    "        #print(t,keypoint[0])\n",
    "\n",
    "        predicted_x = filter_x.predict(x=np.array([t,int(scores[i]>5.5)]))\n",
    "        predicted_y = filter_y.predict(x=np.array([t,int(scores[i]>5.5)]))\n",
    "        predicted_s = filter_z.predict(x=np.array([t,int(scores[i]>5.5)]))\n",
    "\n",
    "        filter_x.adapt(x=np.array([t,int(scores[i]>5.5)]), d=keypoint[0])\n",
    "        filter_y.adapt(x=np.array([t,int(scores[i]>5.5)]), d=keypoint[1])\n",
    "        filter_z.adapt(x=np.array([t,int(scores[i]>5.5)]), d=scores[i])\n",
    "        \n",
    "        #print(\"predicted_x:\",predicted_x)\n",
    "        keypoints_pred[i] = [predicted_x, predicted_y]\n",
    "        scores_pred[i]    = predicted_s\n",
    "        \"\"\"\n",
    "        if scores[i]<5.5:\n",
    "            # Si el keypoint es 0, utilizar la predicción del filtro adaptativo\n",
    "            keypoints_pred[i] = [predicted_x, predicted_y]\n",
    "        else:\n",
    "            # Si el keypoint es válido, actualizar el filtro adaptativo\n",
    "            keypoints_pred[i] = [keypoint[0],keypoint[1]]\n",
    "        \"\"\"\n",
    "    return keypoints_pred,scores_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id = 99\n",
    "\n",
    "# Inicializar los filtros adaptativos para cada keypoint\n",
    "num_keypoints = 1  # o 266\n",
    "adaptive_filters = initialize_adaptive_filter(num_keypoints,mu=1.05)\n",
    "\n",
    "# Coordinates x,y of index 99 and scores of pose estimation model\n",
    "points_x      = list_keypoints[:,id,0] \n",
    "points_y      = list_keypoints[:,id,1]\n",
    "scores_points = list_scores[:,id]\n",
    "\n",
    "points_x_pred      = np.zeros_like(points_x)\n",
    "points_y_pred      = np.zeros_like(points_y)\n",
    "scores_points_pred = np.zeros_like(scores_points)\n",
    "\n",
    "# Simulation of filtering of coordinates frame by frame\n",
    "for i in range(len(points_x)):\n",
    "    keyp_aux   = [[points_x[i],points_y[i]]]\n",
    "    scores_aux = [scores_points[i]]\n",
    "    \n",
    "    # filtering data (TO DO)\n",
    "    keyp_pred,scores_pred = track_keypoints(i,keyp_aux,scores_aux, adaptive_filters)\n",
    "    \n",
    "    # save data predicted\n",
    "    points_x_pred[i] = keyp_pred[0][0]\n",
    "    points_y_pred[i] = keyp_pred[0][1]\n",
    "    scores_points_pred[i] = scores_pred[0]\n",
    "\n",
    "\n",
    "n = len(points_x)\n",
    "fig = plt.figure(figsize=(15,6))\n",
    "\n",
    "plt.subplot(3,1,1)\n",
    "plt.plot(points_x, '-ob', label='Original X Points')\n",
    "plt.plot(points_x_pred, '-or', alpha=0.5, label='Filtered X Points')\n",
    "plt.xticks(np.arange(0, n, step=2))\n",
    "plt.xlabel('Point Index')\n",
    "plt.ylabel('X Coordinate')\n",
    "plt.title('Pose Signal Filtering - X Coordinate')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(3,1,2)\n",
    "plt.plot(points_y, '-ob', label='Original Y Points')\n",
    "plt.plot(points_y_pred, '-or', alpha=0.5, label='Filtered Y Points')\n",
    "plt.xticks(np.arange(0, n, step=2))\n",
    "plt.xlabel('Point Index')\n",
    "plt.ylabel('Y Coordinate')\n",
    "plt.title('Pose Signal Filtering - Y Coordinate')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(3,1,3)\n",
    "plt.plot(scores_points, '-ob', label='Original Scores')\n",
    "plt.plot(scores_points_pred, '-or', alpha=0.5, label='Filtered Scores')\n",
    "plt.plot([0, len(points_x)], [5.5, 5.5], '-g', label='Score Threshold')\n",
    "plt.xticks(np.arange(0, n, step=2))\n",
    "plt.xlabel('Point Index')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Score Signal Filtering')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# define a video capture object \n",
    "#vid = cv2.VideoCapture(\"/media/cristian/12FF1F6D0CD48422/Research/Gloss/Gloss/Datasets/wlasl-complete/videos/17165.mp4\")#68508.mp4\")#54563.mp4\")#68914.mp4\") \n",
    "\n",
    "\n",
    "num_keypoints = 133\n",
    "\n",
    "\n",
    "adaptive_filters = initialize_adaptive_filter(num_keypoints,mu=1.05)\n",
    "\n",
    "\n",
    "vid = cv2.VideoCapture(\"/media/cristian/12FF1F6D0CD48422/Research/Gloss/Gloss/Datasets/WLASL/wlasl-complete-21k/videos/17165.mp4\")\n",
    "                       \n",
    "#vid = cv2.VideoCapture(0)\n",
    "cv2.namedWindow(\"frame\", cv2.WINDOW_NORMAL) \n",
    "\n",
    "list_keypoints = []\n",
    "list_scores = []\n",
    "cnt = 0\n",
    "\n",
    "while(True): \n",
    "    \n",
    "    ret, frame = vid.read() \n",
    "    if ret is None or frame is None:\n",
    "        break\n",
    "    \n",
    "    frame = cv2.resize(frame,(640,480))\n",
    "    \n",
    "    try:\n",
    "        frame_rgb = cv2.cvtColor(frame,cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        \n",
    "        #POSE ESTIMATION MODEL\n",
    "        keypoints, scores  = model.predict(frame_rgb)\n",
    "        list_keypoints.append(keypoints[0,:,:])\n",
    "        list_scores.append(scores[0,:])\n",
    "        \n",
    "        # FILTERING\n",
    "        keypoints_filtered       = np.zeros_like([[[0,0] for i in range(num_keypoints)]])\n",
    "        scores_filtered          = np.zeros_like([[0 for i in range(num_keypoints)]])\n",
    "        keyp_pred,scores_pred = track_keypoints(cnt,keypoints[0,:,:],scores[0,:], adaptive_filters)            \n",
    "        keypoints_filtered[0]  = keyp_pred\n",
    "        scores_filtered[0]     = scores_pred\n",
    "        \n",
    "        \n",
    "        #DRAWING        \n",
    "        frame_original = draw_skeleton(copy.deepcopy(frame), keypoints_filtered, scores_filtered, kpt_thr=kpt_thr,\n",
    "                                    line_width=1,radius=1)\n",
    "        \n",
    "       \n",
    "        score_value = np.round(scores[0,99],2)\n",
    "        key_p1 = np.round(keypoints[0,99,0],2)\n",
    "        key_p2 = np.round(keypoints[0,99,1],2)\n",
    "        cnt+=1        \n",
    "\n",
    "        frame_original = cv2.putText(frame_original, \n",
    "                    f'point 99:i={cnt},s={str(score_value)[:4]},k={str(key_p1)[:6]},{str(key_p2)[:6]}', \n",
    "                    (10, 30) ,cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 0) , 2, cv2.LINE_AA) \n",
    "        \n",
    "        cv2.imshow('frame', frame_original) \n",
    "        #cv2.waitKey()\n",
    "\n",
    "        # DELAY TO SEE THE VIDEO SLOWLY\n",
    "        time.sleep(0.2)\n",
    "        \n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'): \n",
    "            break\n",
    "    except:\n",
    "        print(traceback.format_exc())\n",
    "        break\n",
    "    \n",
    "# After the loop release the cap object \n",
    "vid.release() \n",
    "# Destroy all the windows \n",
    "cv2.destroyAllWindows() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "end_slr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
